{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d35dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All dependencies imported successfully!\n",
      "üìÅ Make sure your PDF files are in the './gnidp_pdfs/' directory\n",
      "ü§ñ Make sure Ollama is running with: ollama serve\n",
      "üì¶ Make sure gemma3:4b model is pulled: ollama pull gemma3:4b\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# MODULE 1: DEPENDENCIES AND IMPORTS\n",
    "# =====================================\n",
    "# Run this cell first to install and import all required dependencies\n",
    "\n",
    "# Install dependencies (run once)\n",
    "# !pip install langchain langchain-community chromadb pypdf sentence-transformers faiss-cpu numpy huggingface-hub requests torch\n",
    "# ollama run qwen3:0.6b \n",
    "\n",
    "import os\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Core libraries\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ All dependencies imported successfully!\")\n",
    "print(\"üìÅ Make sure your PDF files are in the './gnidp_pdfs/' directory\")\n",
    "print(\"ü§ñ Make sure Ollama is running with: ollama serve\")\n",
    "print(\"üì¶ Make sure gemma3:4b model is pulled: ollama pull gemma3:4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6be2cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for caching\n",
    "from diskcache import Cache\n",
    "from functools import lru_cache\n",
    "from typing import List, Dict, Any, Optional  # Add Optional to existing imports\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "# Cache module\n",
    "class QueryCache:\n",
    "    \"\"\"Handles multi-level caching for GNIDP RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"cache\", ttl: int = 3600):\n",
    "        self.memory_cache: Dict[str, Any] = {}\n",
    "        self.cache_dir = cache_dir\n",
    "        self.ttl = ttl  # Time-to-live in seconds\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        self.disk_cache = Cache(cache_dir)\n",
    "        \n",
    "    def _generate_cache_key(self, query: str) -> str:\n",
    "        \"\"\"Generate consistent cache key from query\"\"\"\n",
    "        return hashlib.sha256(query.lower().strip().encode()).hexdigest()\n",
    "    \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def _get_from_memory(self, cache_key: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get result from memory cache with LRU strategy\"\"\"\n",
    "        result = self.memory_cache.get(cache_key)\n",
    "        if result and (time.time() - result['timestamp']) < self.ttl:\n",
    "            return result['data']\n",
    "        return None\n",
    "    \n",
    "    def get(self, query: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get cached result using multi-level cache strategy\"\"\"\n",
    "        cache_key = self._generate_cache_key(query)\n",
    "        \n",
    "        # Try memory cache first\n",
    "        result = self._get_from_memory(cache_key)\n",
    "        if result:\n",
    "            return result\n",
    "        \n",
    "        # Try disk cache\n",
    "        result = self.disk_cache.get(cache_key)\n",
    "        if result:\n",
    "            # Promote to memory cache\n",
    "            self.set(query, result)\n",
    "            return result\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def set(self, query: str, result: Dict[str, Any]) -> None:\n",
    "        \"\"\"Store result in both memory and disk cache\"\"\"\n",
    "        cache_key = self._generate_cache_key(query)\n",
    "        \n",
    "        # Memory cache\n",
    "        self.memory_cache[cache_key] = {\n",
    "            'data': result,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        \n",
    "        # Disk cache\n",
    "        self.disk_cache.set(cache_key, result, expire=self.ttl)\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear all caches\"\"\"\n",
    "        self.memory_cache.clear()\n",
    "        self.disk_cache.clear()\n",
    "        \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        return {\n",
    "            'memory_cache_size': len(self.memory_cache),\n",
    "            'disk_cache_size': len(self.disk_cache),\n",
    "            'cache_dir': self.cache_dir,\n",
    "            'ttl': self.ttl\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4794f2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n",
      "üìÅ PDF Directory: C:\\Users\\ACER\\Documents\\NIC_intern\\Little Andaman\\D_set\n",
      "ü§ñ Ollama Model: qwen3:0.6b\n",
      "üß† Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "üìÑ Chunk Size: 500\n",
      "üóÉÔ∏è Vector Store: faiss\n",
      "üìö Found 6 PDF files in directory\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# MODULE 2: CONFIGURATION SETTINGS\n",
    "# =====================================\n",
    "# Configure all parameters for the RAG system\n",
    "\n",
    "class GNIDPConfig:\n",
    "    \"\"\"Configuration class for GNIDP RAG Chatbot\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # File paths\n",
    "        self.PDF_DIRECTORY = r\"C:\\Users\\ACER\\Documents\\NIC_intern\\Little Andaman\\D_set\"  # UPDATE THIS PATH TO YOUR PDF DIRECTORY\n",
    "        self.VECTORSTORE_DIR = r\"C:\\Users\\ACER\\Documents\\NIC_intern\\Little Andaman\\V_set\"  # Vector store persistence directory\n",
    "        self.VECTORSTORE_FILENAME = \"LA_vectorstore\"  # Removed .faiss extension\n",
    "        self.REBUILD_VECTORSTORE = False # Changed to True to force rebuild\n",
    "        \n",
    "        # Model settings\n",
    "        self.OLLAMA_MODEL = \"qwen3:0.6b\"  # Ollama model for LLM\n",
    "        self.EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        \n",
    "        # Performance-optimized chunking parameters\n",
    "        self.CHUNK_SIZE = 500  # Down from 1000\n",
    "        self.CHUNK_OVERLAP = 20  # Reduced from 50\n",
    "        \n",
    "        # Vector store settings\n",
    "        self.VECTORSTORE_TYPE = \"faiss\"  # \"faiss\" for speed, \"chroma\" for persistence\n",
    "        self.RETRIEVAL_K = 1  # Reduced from 3 for faster retrieval\n",
    "        \n",
    "        # LLM parameters\n",
    "        self.TEMPERATURE = 0.2\n",
    "        self.TOP_K = 1\n",
    "        self.TOP_P = 0.9\n",
    "        self.NUM_CTX = 512  # Reduced context window for faster processing\n",
    "        \n",
    "        # GNIDP-specific keywords for filtering\n",
    "        self.GNIDP_KEYWORDS = [\n",
    "            # Island Names & Geography\n",
    "    'little andaman', 'little andaman island', 'south andaman', 'andaman islands',\n",
    "    'bay of bengal', 'andaman sea', 'ten degree channel', 'car nicobar',\n",
    "    \n",
    "    # Administrative & Locations\n",
    "    'hut bay', 'dugong creek', 'butler bay', 'netaji nagar', 'harminder bay',\n",
    "    'jackson creek', 'whisper wave', 'kala pathar', 'white surf', 'lalaji bay',\n",
    "    'rampur', 'govind nagar', 'om shanti', 'khari nadi', 'bumila',\n",
    "    \n",
    "    # Beaches & Coastal Areas\n",
    "    'butler bay beach', 'kala pathar beach', 'white surf beach', 'whisper wave beach',\n",
    "    'netaji nagar beach', 'harminder bay beach', 'lalaji bay beach', 'om shanti beach',\n",
    "    'beach', 'coastline', 'shore', 'bay', 'creek', 'surf point',\n",
    "    \n",
    "    # Indigenous Communities & Tribes\n",
    "    'onge', 'onge tribe', 'indigenous people', 'tribal community', 'primitive tribe',\n",
    "    'particularly vulnerable tribal group', 'pvtg', 'aboriginal', 'native population',\n",
    "    'tribal rights', 'tribal land', 'forest rights', 'customary rights',\n",
    "    \n",
    "    # Wildlife & Biodiversity\n",
    "    'elephant', 'asian elephant', 'wild elephant', 'elephant corridor',\n",
    "    'saltwater crocodile', 'estuarine crocodile', 'sea turtle', 'olive ridley',\n",
    "    'leatherback turtle', 'green turtle', 'hawksbill turtle', 'turtle nesting',\n",
    "    'dugong', 'dolphins', 'whales', 'coral reef', 'mangroves', 'tropical forest',\n",
    "    'endemic species', 'biodiversity', 'wildlife sanctuary', 'marine life',\n",
    "    'bird watching', 'migratory birds', 'nesting sites', 'breeding grounds',\n",
    "    \n",
    "    # Flora & Forest\n",
    "    'tropical rainforest', 'evergreen forest', 'littoral forest', 'mangrove forest',\n",
    "    'padauk', 'gurjan', 'mahua', 'bamboo', 'rattan', 'medicinal plants',\n",
    "    'timber', 'forest cover', 'deforestation', 'forest conservation',\n",
    "    \n",
    "    # Marine & Aquatic\n",
    "    'coral reef', 'coral bleaching', 'marine ecosystem', 'seagrass beds',\n",
    "    'fishing', 'marine protected area', 'coastal zone', 'tidal zone',\n",
    "    'marine biodiversity', 'reef fish', 'shark', 'ray', 'barracuda',\n",
    "    \n",
    "    # Tourism & Activities\n",
    "    'eco-tourism', 'beach tourism', 'adventure tourism', 'surfing', 'scuba diving',\n",
    "    'snorkeling', 'water sports', 'jungle trekking', 'elephant safari',\n",
    "    'nature walk', 'bird watching', 'fishing', 'boating', 'swimming',\n",
    "    'tourist spots', 'tourist attractions', 'accommodation', 'resorts', 'hotels',\n",
    "    \n",
    "    # Transportation & Connectivity\n",
    "    'helicopter', 'helicopter service', 'ship', 'ferry', 'boat service',\n",
    "    'inter-island connectivity', 'port blair', 'hut bay jetty', 'landing ground',\n",
    "    'road network', 'transportation', 'accessibility', 'remote location',\n",
    "    \n",
    "    # Development & Infrastructure\n",
    "    'infrastructure development', 'road construction', 'jetty development',\n",
    "    'tourism infrastructure', 'sustainable development', 'carrying capacity',\n",
    "    'development pressure', 'urbanization', 'settlement', 'village development',\n",
    "    \n",
    "    # Environmental Concerns\n",
    "    'environmental impact', 'climate change', 'sea level rise', 'coastal erosion',\n",
    "    'tsunami', 'cyclone', 'natural disaster', 'vulnerability', 'adaptation',\n",
    "    'conservation', 'protection', 'sustainable use', 'ecosystem services',\n",
    "    'environmental degradation', 'pollution', 'waste management',\n",
    "    \n",
    "    # Research & Conservation\n",
    "    'research station', 'scientific study', 'ecological research', 'marine research',\n",
    "    'conservation project', 'wildlife protection', 'habitat conservation',\n",
    "    'species protection', 'breeding program', 'monitoring', 'survey',\n",
    "    \n",
    "    # Cultural & Heritage\n",
    "    'tribal culture', 'traditional knowledge', 'cultural heritage', 'folklore',\n",
    "    'traditional practices', 'cultural preservation', 'ethnography',\n",
    "    'anthropological study', 'cultural rights', 'cultural identity',\n",
    "    \n",
    "    # Agriculture & Livelihood\n",
    "    'coconut plantation', 'rubber plantation', 'arecanut', 'paddy cultivation',\n",
    "    'kitchen garden', 'shifting cultivation', 'traditional agriculture',\n",
    "    'livelihood', 'subsistence farming', 'fishing community', 'forest produce',\n",
    "    \n",
    "    # Government & Administration\n",
    "    'andaman nicobar administration', 'district collector', 'forest department',\n",
    "    'tribal welfare', 'panchayat', 'local government', 'revenue village',\n",
    "    'land records', 'government schemes', 'welfare programs',\n",
    "    \n",
    "    # Legal & Policy\n",
    "    'forest rights act', 'tribal rights', 'environmental clearance',\n",
    "    'coastal regulation zone', 'crz', 'wildlife protection act',\n",
    "    'forest conservation act', 'island protection regulation', 'land use policy',\n",
    "    \n",
    "    # Challenges & Issues\n",
    "    'human-elephant conflict', 'human-wildlife conflict', 'encroachment',\n",
    "    'illegal logging', 'poaching', 'overfishing', 'tourism pressure',\n",
    "    'waste disposal', 'sewage treatment', 'water scarcity', 'power supply',\n",
    "    \n",
    "    # Natural Resources\n",
    "    'freshwater', 'groundwater', 'natural springs', 'water resources',\n",
    "    'renewable energy', 'solar energy', 'wind energy', 'natural gas',\n",
    "    'mineral resources', 'sand mining', 'stone quarrying',\n",
    "    \n",
    "    # Weather & Climate\n",
    "    'tropical climate', 'monsoon', 'rainfall', 'humidity', 'temperature',\n",
    "    'seasonal variation', 'dry season', 'wet season', 'weather pattern',\n",
    "    \n",
    "    # Specific Projects & Initiatives\n",
    "    'eco-development', 'community-based tourism', 'sustainable tourism',\n",
    "    'conservation education', 'awareness program', 'capacity building',\n",
    "    'participatory management', 'stakeholder engagement',\n",
    "    \n",
    "    # Infrastructure & Facilities Count/Numbers\n",
    "    'number of buses', 'bus service', 'public transport', 'bus routes', 'bus frequency',\n",
    "    'how many buses', 'bus schedule', 'bus timings', 'local transport',\n",
    "    \n",
    "    'number of hospitals', 'hospital facilities', 'medical facilities', 'healthcare',\n",
    "    'how many hospitals', 'primary health center', 'phc', 'dispensary', 'clinic',\n",
    "    'medical services', 'emergency medical', 'ambulance service',\n",
    "    \n",
    "    'number of schools', 'educational institutions', 'primary school', 'high school',\n",
    "    'how many schools', 'education facilities', 'school enrollment', 'teachers',\n",
    "    'educational infrastructure', 'literacy rate', 'anganwadi', 'pre-school',\n",
    "    \n",
    "    'number of shops', 'retail stores', 'market', 'shopping facilities', 'grocery stores',\n",
    "    'how many stores', 'local market', 'weekly market', 'cooperative society',\n",
    "    'fair price shop', 'ration shop', 'commercial establishments',\n",
    "    \n",
    "    'number of atms', 'banking facilities', 'bank branches', 'how many atms',\n",
    "    'financial services', 'post office', 'money transfer', 'digital banking',\n",
    "    'cooperative bank', 'self help groups', 'microfinance',\n",
    "    \n",
    "    'number of villages', 'settlements', 'hamlets', 'revenue villages',\n",
    "    'how many villages', 'village population', 'household count', 'families',\n",
    "    'inhabited villages', 'tribal settlements', 'forest villages',\n",
    "    \n",
    "    'veterinary hospital', 'vet clinic', 'animal hospital', 'livestock care',\n",
    "    'veterinary services', 'animal health', 'cattle treatment', 'pet care',\n",
    "    'veterinary doctor', 'animal welfare', 'vaccination program',\n",
    "    \n",
    "    'fire station', 'fire brigade', 'fire service', 'emergency services',\n",
    "    'fire safety', 'disaster management', 'rescue services', 'fire department',\n",
    "    \n",
    "    # Population & Demographics\n",
    "    'population', 'total population', 'population density', 'demographic data',\n",
    "    'population census', 'household size', 'age distribution', 'gender ratio',\n",
    "    'tribal population', 'onge population', 'settler population', 'migrant population',\n",
    "    'population growth', 'birth rate', 'death rate', 'literacy statistics',\n",
    "    \n",
    "    # Area & Land Measurements\n",
    "    'total area', 'land area', 'geographical area', 'square kilometers', 'sq km',\n",
    "    'hectares', 'area in hectares', 'island area', 'land measurement',\n",
    "    'total land', 'usable land', 'cultivable land', 'agricultural area',\n",
    "    \n",
    "    'forest area', 'forest cover', 'forest percentage', 'wooded area',\n",
    "    'reserve forest', 'protected forest', 'forest land', 'tree cover',\n",
    "    'forest density', 'forest statistics', 'deforestation rate',\n",
    "    \n",
    "    'coastal area', 'shoreline length', 'coastline', 'beach area',\n",
    "    'marine area', 'territorial waters', 'exclusive economic zone',\n",
    "    \n",
    "    # Road Infrastructure & Distances\n",
    "    'road length', 'total road length', 'road network', 'road infrastructure',\n",
    "    'paved roads', 'unpaved roads', 'motorable roads', 'all weather roads',\n",
    "    'road condition', 'road connectivity', 'highway', 'state highway',\n",
    "    'village roads', 'forest roads', 'beach roads',\n",
    "    \n",
    "    'distance to port blair', 'distance from port blair', 'distance to hut bay',\n",
    "    'distance between villages', 'travel distance', 'road distance',\n",
    "    'aerial distance', 'how far', 'travel time', 'journey time',\n",
    "    'distance to airport', 'distance to jetty', 'distance to hospital',\n",
    "    'distance to school', 'nearest town', 'nearest city',\n",
    "    \n",
    "    # Utilities & Services Count\n",
    "    'electricity connections', 'power supply', 'solar installations', 'generators',\n",
    "    'water supply', 'water connections', 'bore wells', 'hand pumps',\n",
    "    'water treatment plants', 'sewage treatment', 'waste management',\n",
    "    \n",
    "    'telephone connections', 'mobile towers', 'internet connectivity',\n",
    "    'broadband', 'digital infrastructure', 'communication facilities',\n",
    "    \n",
    "    'police station', 'police post', 'security', 'law and order',\n",
    "    'coast guard', 'border security', 'checkpoints',\n",
    "    \n",
    "    # Specific Facility Numbers/Statistics\n",
    "    'number of beds', 'hospital beds', 'bed capacity', 'icu beds',\n",
    "    'number of doctors', 'medical staff', 'nurses', 'paramedics',\n",
    "    'number of teachers', 'student teacher ratio', 'enrollment numbers',\n",
    "    'number of vehicles', 'registered vehicles', 'two wheelers', 'four wheelers',\n",
    "    'number of boats', 'fishing boats', 'motorboats', 'traditional boats',\n",
    "    \n",
    "    # Economic Data\n",
    "    'per capita income', 'household income', 'poverty rate', 'employment rate',\n",
    "    'unemployment', 'income statistics', 'economic indicators', 'gdp',\n",
    "    'budget allocation', 'government expenditure', 'development funds',\n",
    "    \n",
    "    # Fair Price Shops & Public Distribution System\n",
    "    'fair price shop', 'fps', 'ration shop', 'public distribution system', 'pds',\n",
    "    'number of fps', 'how many fair price shops', 'ration dealers', 'pds outlets',\n",
    "    'subsidized food', 'food grains', 'kerosene', 'sugar distribution',\n",
    "    'monthly quota', 'food security', 'essential commodities',\n",
    "    \n",
    "    # Cardholders & Beneficiaries\n",
    "    'ration cardholders', 'apl cardholders', 'bpl cardholders', 'aay cardholders',\n",
    "    'above poverty line', 'below poverty line', 'antyodaya anna yojana',\n",
    "    'number of beneficiaries', 'eligible families', 'cardholder statistics',\n",
    "    'beneficiary count', 'scheme beneficiaries', 'welfare recipients',\n",
    "    'food subsidy beneficiaries', 'targeted beneficiaries', 'coverage ratio',\n",
    "    \n",
    "    'aadhar cardholders', 'voter id cards', 'identity cards', 'documentation',\n",
    "    'jan aushadhi beneficiaries', 'health insurance beneficiaries',\n",
    "    'pension beneficiaries', 'scholarship recipients', 'self help group members',\n",
    "    \n",
    "    # Electricity Department & Infrastructure\n",
    "    'electricity department', 'power department', 'electrical division',\n",
    "    'power generation capacity', 'installed capacity', 'power consumption',\n",
    "    'electricity production', 'power demand', 'load shedding', 'power cuts',\n",
    "    'electrical connections', 'domestic connections', 'commercial connections',\n",
    "    'industrial connections', 'street lighting', 'power distribution',\n",
    "    'transformer capacity', 'grid capacity', 'power lines', 'electrical poles',\n",
    "    'meter readings', 'electricity bills', 'tariff rates', 'power subsidy',\n",
    "    'renewable energy capacity', 'solar power generation', 'wind power',\n",
    "    'diesel generators', 'backup power', 'uninterrupted power supply',\n",
    "    'power outages', 'electrical faults', 'maintenance schedule',\n",
    "    \n",
    "    # Water Department & Supply System\n",
    "    'water department', 'public health engineering', 'phed', 'water supply department',\n",
    "    'water treatment plant', 'wtp capacity', 'water production capacity',\n",
    "    'daily water production', 'water storage capacity', 'reservoir capacity',\n",
    "    'overhead tank capacity', 'underground tank capacity', 'water distribution',\n",
    "    'pipe network', 'water connections', 'household water connections',\n",
    "    'commercial water connections', 'institutional connections',\n",
    "    'water supply hours', 'water quality', 'water testing', 'bacteriological testing',\n",
    "    'chemical testing', 'water treatment', 'chlorination', 'filtration',\n",
    "    'water tanker supply', 'emergency water supply', 'water shortage',\n",
    "    'water conservation', 'rainwater harvesting', 'water recycling',\n",
    "    'sewage treatment capacity', 'wastewater treatment', 'sewage disposal',\n",
    "    \n",
    "    # Fresh Water Resources & Capacity\n",
    "    'freshwater sources', 'freshwater availability', 'freshwater reserves',\n",
    "    'groundwater', 'groundwater table', 'water table level', 'aquifer capacity',\n",
    "    'natural springs', 'stream flow', 'surface water', 'water bodies',\n",
    "    'pond capacity', 'lake capacity', 'river flow', 'creek flow',\n",
    "    'well capacity', 'bore well depth', 'bore well yield', 'water extraction',\n",
    "    'sustainable yield', 'water recharge', 'monsoon recharge', 'infiltration rate',\n",
    "    'water balance', 'water budget', 'water stress', 'water scarcity',\n",
    "    'per capita water availability', 'daily water requirement', 'water demand',\n",
    "    'drinking water', 'potable water', 'safe drinking water', 'water purification',\n",
    "    'water storage tanks', 'community tanks', 'individual storage',\n",
    "    \n",
    "    # Population Sustenance & Carrying Capacity\n",
    "    'carrying capacity', 'population carrying capacity', 'sustainable population',\n",
    "    'population pressure', 'overpopulation', 'population limit', 'ecological footprint',\n",
    "    'resource availability per capita', 'land per capita', 'water per capita',\n",
    "    'food security', 'food self sufficiency', 'food production capacity',\n",
    "    'agricultural productivity', 'crop yield', 'food grains production',\n",
    "    'fish production', 'protein availability', 'nutritional security',\n",
    "    'calorie availability', 'malnutrition rate', 'undernourishment',\n",
    "    'food distribution', 'food access', 'food affordability', 'food wastage',\n",
    "    \n",
    "    'livelihood sustainability', 'employment capacity', 'job opportunities',\n",
    "    'income generation', 'economic sustainability', 'resource depletion',\n",
    "    'environmental degradation', 'ecological balance', 'natural resource management',\n",
    "    'waste generation', 'waste disposal capacity', 'pollution load',\n",
    "    'carbon footprint', 'environmental impact per capita',\n",
    "    \n",
    "    'healthcare capacity', 'patient load', 'doctor patient ratio',\n",
    "    'educational capacity', 'student capacity', 'infrastructure load',\n",
    "    'housing capacity', 'accommodation availability', 'settlement density',\n",
    "    'transportation capacity', 'traffic load', 'road capacity',\n",
    "    \n",
    "    # Government Schemes & Programs\n",
    "    'mgnrega beneficiaries', 'pmay beneficiaries', 'pradhan mantri awas yojana',\n",
    "    'swachh bharat mission', 'toilet construction', 'ujjwala yojana',\n",
    "    'lpg connections', 'ayushman bharat', 'health insurance coverage',\n",
    "    'pradhan mantri jan dhan yojana', 'bank account holders',\n",
    "    'digital india', 'skill development', 'startup schemes',\n",
    "    'kisan credit cards', 'crop insurance', 'pension schemes',\n",
    "    'widow pension', 'disability pension', 'old age pension',\n",
    "    \n",
    "    # Smart Island & NITI Aayog Development Project\n",
    "    'smart island project', 'smart island initiative', 'digital island', 'smart city',\n",
    "    'niti aayog proposal', 'niti aayog project', 'niti aayog vision document',\n",
    "    'sustainable development of little andaman island', 'vision document',\n",
    "    'little andaman development plan', 'little andaman project', 'mega project',\n",
    "    'megacity plan', 'megacity project', 'smart island development',\n",
    "    \n",
    "    # Greenfield Coastal City Development\n",
    "    'greenfield coastal city', 'new coastal city', 'planned city', 'modern city',\n",
    "    'urban development', 'city planning', 'master plan', 'development zones',\n",
    "    'free trade zone', 'ftz', 'special economic zone', 'sez', 'trade hub',\n",
    "    'maritime hub', 'startup hub', 'financial district', 'business district',\n",
    "    'commercial zone', 'industrial zone', 'residential zone', 'tourism zone',\n",
    "    \n",
    "    # Three Development Zones\n",
    "    'zone 1', 'zone 2', 'zone 3', 'development zones', 'zoning plan',\n",
    "    'financial district', 'medi metropolis', 'aerocity', 'hospital district',\n",
    "    'leisure zone', 'movie metropolis', 'film city', 'entertainment district',\n",
    "    'residential areas', 'housing development', 'township development',\n",
    "    \n",
    "    # Infrastructure Development Components\n",
    "    'underwater resorts', 'underwater hotels', 'marine tourism', 'luxury resorts',\n",
    "    'casinos', 'gaming', 'entertainment complex', 'golf courses', 'sports facilities',\n",
    "    'convention centers', 'conference facilities', 'exhibition centers',\n",
    "    'cruise terminals', 'marina development', 'yacht harbors', 'water sports facilities',\n",
    "    'theme parks', 'amusement parks', 'recreational facilities',\n",
    "    \n",
    "    # Connectivity & Transportation\n",
    "    'airport development', 'runway expansion', 'aviation infrastructure',\n",
    "    'seaplane services', 'helicopter services', 'inter-island connectivity',\n",
    "    'ferry services', 'high-speed connectivity', 'broadband infrastructure',\n",
    "    'digital connectivity', 'submarine cables', 'satellite connectivity',\n",
    "    'road network expansion', 'highway development', 'bridge construction',\n",
    "    \n",
    "    # Comparison with Global Cities\n",
    "    'singapore model', 'hong kong model', 'compete with singapore', 'compete with hong kong',\n",
    "    'international trade hub', 'global city', 'world-class infrastructure',\n",
    "    'international standards', 'global competitiveness', 'trade activity',\n",
    "    \n",
    "    # Strategic Location & Geopolitics\n",
    "    'strategic location', 'indian ocean region', 'ior', 'geopolitical importance',\n",
    "    'maritime security', 'naval base', 'strategic assets', 'security concerns',\n",
    "    'china containment', 'look east policy', 'act east policy', 'indo-pacific',\n",
    "    \n",
    "    # Project Status & Timeline\n",
    "    'project status', 'project suspended', 'project cancelled', 'project timeline',\n",
    "    'implementation plan', 'project phases', 'development stages', 'project funding',\n",
    "    'investment requirements', 'budget allocation', 'cost estimates', 'financial viability',\n",
    "    \n",
    "    # Technology & Innovation\n",
    "    'smart technology', 'iot implementation', 'artificial intelligence', 'digital governance',\n",
    "    'e-governance', 'smart utilities', 'smart grid', 'renewable energy integration',\n",
    "    'waste management technology', 'water management systems', 'traffic management',\n",
    "    'smart mobility', 'electric vehicles', 'sustainable transport', 'green technology',\n",
    "    \n",
    "    # Environmental Concerns & Sustainability\n",
    "    'environmental impact assessment', 'eia', 'environmental clearance',\n",
    "    'forest clearance', 'coastal regulation zone clearance', 'crz clearance',\n",
    "    'biodiversity impact', 'ecological impact', 'carbon footprint', 'green development',\n",
    "    'sustainable tourism', 'eco-friendly development', 'environmental monitoring',\n",
    "    'pollution control', 'waste management', 'sewage treatment', 'water treatment',\n",
    "    \n",
    "    # Tribal & Social Impact\n",
    "    'onge tribe impact', 'tribal displacement', 'indigenous rights', 'tribal consultation',\n",
    "    'rehabilitation', 'resettlement', 'social impact assessment', 'community participation',\n",
    "    'stakeholder engagement', 'public consultation', 'consent process',\n",
    "    'cultural preservation', 'traditional livelihood', 'tribal welfare',\n",
    "    \n",
    "    # Opposition & Concerns\n",
    "    'conservationist concerns', 'environmental opposition', 'tribal rights activists',\n",
    "    'project criticism', 'sustainability concerns', 'ecological concerns',\n",
    "    'development vs conservation', 'protests', 'legal challenges', 'court cases',\n",
    "    'ngo opposition', 'civil society concerns', 'expert opinions',\n",
    "    \n",
    "    # Economic Aspects\n",
    "    'economic development', 'gdp contribution', 'employment generation', 'job creation',\n",
    "    'tourism revenue', 'trade revenue', 'foreign investment', 'fdi',\n",
    "    'public-private partnership', 'ppp model', 'investment opportunities',\n",
    "    'economic growth', 'revenue generation', 'tax revenue', 'export earnings',\n",
    "    \n",
    "    # Comparative Statistics\n",
    "    'statistics', 'data', 'figures', 'numbers', 'count', 'total number',\n",
    "    'how many', 'quantity', 'availability', 'capacity', 'coverage',\n",
    "    'percentage', 'ratio', 'rate', 'density', 'frequency', 'adequacy',\n",
    "    'sufficiency', 'shortfall', 'surplus', 'deficit', 'utilization rate'\n",
    "        ]\n",
    "\n",
    "        # Cache settings\n",
    "        self.CACHE_DIR = r\"C:\\Users\\ACER\\Documents\\NIC_intern\\Little Andaman\\cache\"\n",
    "        self.CACHE_TTL = 3600  # Cache time-to-live in seconds\n",
    "        self.ENABLE_CACHE = True\n",
    "        self.MAX_MEMORY_CACHE = 1000  # Maximum items in memory cache\n",
    "\n",
    "# Initialize configuration\n",
    "config = GNIDPConfig()\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n",
    "print(f\"üìÅ PDF Directory: {config.PDF_DIRECTORY}\")\n",
    "print(f\"ü§ñ Ollama Model: {config.OLLAMA_MODEL}\")\n",
    "print(f\"üß† Embedding Model: {config.EMBEDDING_MODEL}\")\n",
    "print(f\"üìÑ Chunk Size: {config.CHUNK_SIZE}\")\n",
    "print(f\"üóÉÔ∏è Vector Store: {config.VECTORSTORE_TYPE}\")\n",
    "\n",
    "# Verify PDF directory exists\n",
    "if not os.path.exists(config.PDF_DIRECTORY):\n",
    "    print(f\"‚ö†Ô∏è  WARNING: PDF directory '{config.PDF_DIRECTORY}' does not exist!\")\n",
    "    print(f\"üìù Please create the directory and add your PDF files\")\n",
    "else:\n",
    "    pdf_files = [f for f in os.listdir(config.PDF_DIRECTORY) if f.endswith('.pdf')]\n",
    "    print(f\"üìö Found {len(pdf_files)} PDF files in directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4aa2b8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:51:03,021 - INFO - Loading embedding model...\n",
      "2025-06-18 14:51:03,032 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Setting up embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:51:07,976 - INFO - ‚úÖ Quantized embeddings loaded in 4.95 seconds\n",
      "2025-06-18 14:51:07,979 - INFO - üìê Embedding dimension: 384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model ready!\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# MODULE 3: EMBEDDING MODEL SETUP\n",
    "# =====================================\n",
    "# Initialize the embedding model for document vectorization\n",
    "\n",
    "def setup_embeddings(config):\n",
    "    \"\"\"Initialize embedding model - optimized for speed\"\"\"\n",
    "    logger.info(\"Loading embedding model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=config.EMBEDDING_MODEL,\n",
    "            model_kwargs={'device': 'cpu'},  # Use GPU if available\n",
    "            encode_kwargs={\n",
    "                'normalize_embeddings': True,\n",
    "                'batch_size': 32  # Add batch processing\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Test the embedding model\n",
    "        test_text = \"Great Nicobar Island Development Project\"\n",
    "        test_embedding = embeddings.embed_query(test_text)\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        logger.info(f\"‚úÖ Quantized embeddings loaded in {load_time:.2f} seconds\")\n",
    "        logger.info(f\"üìê Embedding dimension: {len(test_embedding)}\")\n",
    "        \n",
    "        return embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error loading embeddings: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Initialize embeddings\n",
    "print(\"üß† Setting up embedding model...\")\n",
    "embeddings = setup_embeddings(config)\n",
    "print(\"‚úÖ Embedding model ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd3ff3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:51:08,042 - INFO - üìù Using existing vector store, skipping document processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Checking document processing requirements...\n",
      "‚úÖ Using existing vector store, document processing skipped!\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# MODULE 4: DOCUMENT LOADING AND PROCESSING\n",
    "# =====================================\n",
    "# Load PDFs and create text chunks\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def load_and_process_documents(config, force_reload=False):\n",
    "    \"\"\"Load PDFs and create text chunks only if needed\"\"\"\n",
    "    \n",
    "    # Check if vector store exists - using simplified path checks\n",
    "    faiss_path = os.path.join(config.VECTORSTORE_DIR, f\"{config.VECTORSTORE_FILENAME}.faiss\")\n",
    "    pkl_path = os.path.join(config.VECTORSTORE_DIR, f\"{config.VECTORSTORE_FILENAME}.pkl\")\n",
    "    \n",
    "    # Skip document processing if vector store exists and no rebuild requested\n",
    "    if os.path.exists(faiss_path) and os.path.exists(pkl_path) and not force_reload and not config.REBUILD_VECTORSTORE:\n",
    "        logger.info(\"üìù Using existing vector store, skipping document processing...\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"üìñ Loading PDF documents...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Check if directory exists and has PDFs\n",
    "        if not os.path.exists(config.PDF_DIRECTORY):\n",
    "            raise FileNotFoundError(f\"PDF directory '{config.PDF_DIRECTORY}' not found!\")\n",
    "        \n",
    "        pdf_files = [f for f in os.listdir(config.PDF_DIRECTORY) if f.endswith('.pdf')]\n",
    "        if not pdf_files:\n",
    "            raise FileNotFoundError(f\"No PDF files found in '{config.PDF_DIRECTORY}'!\")\n",
    "        \n",
    "        logger.info(f\"üìö Found {len(pdf_files)} PDF files\")\n",
    "        \n",
    "        # Load PDFs in parallel\n",
    "        def process_pdf(pdf_path):\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            return loader.load()\n",
    "            \n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(process_pdf, os.path.join(config.PDF_DIRECTORY, pdf)) \n",
    "                      for pdf in pdf_files]\n",
    "            documents = [doc for future in futures for doc in future.result()]\n",
    "            \n",
    "        logger.info(f\"üìÑ Loaded {len(documents)} document pages\")\n",
    "        \n",
    "        # Split documents into chunks with optimized parameters\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=config.CHUNK_SIZE,\n",
    "            chunk_overlap=config.CHUNK_OVERLAP,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \": \", \" \", \"\"],\n",
    "            keep_separator=True,\n",
    "            add_start_index=True,\n",
    "            strip_whitespace=True\n",
    "        )\n",
    "        \n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Display some statistics\n",
    "        total_chars = sum(len(doc.page_content) for doc in texts)\n",
    "        avg_chunk_size = total_chars // len(texts) if texts else 0\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        logger.info(f\"‚úÇÔ∏è  Created {len(texts)} text chunks\")\n",
    "        logger.info(f\"üìä Average chunk size: {avg_chunk_size} characters\")\n",
    "        logger.info(f\"‚è±Ô∏è  Document processing completed in {load_time:.2f} seconds\")\n",
    "        \n",
    "        return texts\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error processing documents: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Process documents only if needed\n",
    "print(\"üìö Checking document processing requirements...\")\n",
    "documents = load_and_process_documents(config)\n",
    "if documents:\n",
    "    print(f\"‚úÖ Successfully processed {len(documents)} text chunks!\")\n",
    "    # Display first chunk as sample\n",
    "    print(\"\\nüìñ Sample chunk:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(documents[0].page_content[:300] + \"...\")\n",
    "    print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"‚úÖ Using existing vector store, document processing skipped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "466cad96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:51:08,073 - INFO - üóÉÔ∏è Loading vector store...\n",
      "2025-06-18 14:51:08,076 - INFO - üîÑ Loading existing vector store...\n",
      "2025-06-18 14:51:08,078 - ERROR - ‚ùå Error loading vector store: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).\n",
      "2025-06-18 14:51:08,079 - INFO - üîÑ Falling back to creating new vector store\n",
      "2025-06-18 14:51:08,081 - ERROR - ‚ùå Vector store not found and no documents provided to create new one\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÉÔ∏è Creating/loading vector database...\n",
      "‚ùå Error with vector store: Vector store files not found and no documents available to create new one\n",
      "üîß Try setting config.REBUILD_VECTORSTORE = True to rebuild the vector store\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# MODULE 5: VECTOR STORE CREATION\n",
    "# =====================================\n",
    "# Create and setup the vector database\n",
    "\n",
    "global vectorstore\n",
    "\n",
    "def create_or_load_vectorstore(texts, embeddings, config):\n",
    "    \"\"\"Create or load vector store with persistence\"\"\"\n",
    "    global vectorstore\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(config.VECTORSTORE_DIR, exist_ok=True)\n",
    "    \n",
    "    logger.info(f\"üóÉÔ∏è {'Creating' if config.REBUILD_VECTORSTORE else 'Loading'} vector store...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    faiss_path = os.path.join(config.VECTORSTORE_DIR, f\"{config.VECTORSTORE_FILENAME}.faiss\")\n",
    "    pkl_path = os.path.join(config.VECTORSTORE_DIR, f\"{config.VECTORSTORE_FILENAME}.pkl\")\n",
    "    \n",
    "    # Try loading existing vector store first if not rebuilding\n",
    "    if os.path.exists(faiss_path) and os.path.exists(pkl_path) and not config.REBUILD_VECTORSTORE:\n",
    "        try:\n",
    "            logger.info(\"üîÑ Loading existing vector store...\")\n",
    "            vectorstore = FAISS.load_local(\n",
    "                folder_path=config.VECTORSTORE_DIR,\n",
    "                embeddings=embeddings,\n",
    "                index_name=config.VECTORSTORE_FILENAME\n",
    "            )\n",
    "            \n",
    "            load_time = time.time() - start_time\n",
    "            logger.info(f\"‚úÖ Vector store loaded successfully in {load_time:.2f} seconds\")\n",
    "            return vectorstore\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error loading vector store: {str(e)}\")\n",
    "            logger.info(\"üîÑ Falling back to creating new vector store\")\n",
    "            config.REBUILD_VECTORSTORE = True\n",
    "    \n",
    "    # Create new vector store if loading failed, rebuild requested, or files don't exist\n",
    "    if texts and (config.REBUILD_VECTORSTORE or not (os.path.exists(faiss_path) and os.path.exists(pkl_path))):\n",
    "        logger.info(\"üèóÔ∏è Creating new vector store...\")\n",
    "        vectorstore = FAISS.from_documents(\n",
    "            texts, \n",
    "            embeddings,\n",
    "            distance_strategy=\"COSINE\"\n",
    "        )\n",
    "            \n",
    "        # Save the vector store\n",
    "        try:\n",
    "            vectorstore.save_local(config.VECTORSTORE_DIR, config.VECTORSTORE_FILENAME)\n",
    "            logger.info(f\"üíæ Vector store saved successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error saving vector store: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        creation_time = time.time() - start_time\n",
    "        logger.info(f\"‚úÖ Vector store creation completed in {creation_time:.2f} seconds\")\n",
    "        \n",
    "        return vectorstore\n",
    "    \n",
    "    logger.error(\"‚ùå Vector store not found and no documents provided to create new one\")\n",
    "    raise FileNotFoundError(\"Vector store files not found and no documents available to create new one\")\n",
    "\n",
    "# Create or load vector store\n",
    "print(\"üóÉÔ∏è Creating/loading vector database...\")\n",
    "\n",
    "try:\n",
    "    vectorstore = create_or_load_vectorstore(documents, embeddings, config)\n",
    "    print(\"‚úÖ Vector store ready for queries!\")\n",
    "\n",
    "    # Test the vector store\n",
    "    print(\"\\nüîç Testing vector store with sample query...\")\n",
    "    test_query = \"environmental impact of GNIDP\"\n",
    "    test_results = vectorstore.similarity_search(test_query, k=2)\n",
    "\n",
    "    print(f\"üìä Found {len(test_results)} relevant documents\")\n",
    "    if test_results:\n",
    "        print(\"\\nüìÑ Most relevant chunk:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(test_results[0].page_content[:200] + \"...\")\n",
    "        print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error with vector store: {str(e)}\")\n",
    "    print(\"üîß Try setting config.REBUILD_VECTORSTORE = True to rebuild the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff4d654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:51:08,104 - INFO - ü§ñ Setting up Ollama LLM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Initializing Ollama LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:51:17,486 - INFO - ‚úÖ LLM setup completed in 9.38 seconds\n",
      "2025-06-18 14:51:17,487 - INFO - üéØ Model: qwen3:0.6b (GPU-accelerated)\n",
      "2025-06-18 14:51:17,487 - INFO - üå°Ô∏è  Temperature: 0.2\n",
      "2025-06-18 14:51:17,488 - INFO - üìù Custom prompt template created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Creating custom prompt template...\n",
      "‚úÖ LLM and prompt template ready!\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# MODULE 6: LLM SETUP AND CONFIGURATION\n",
    "# =====================================\n",
    "# Initialize Ollama LLM and create custom prompt template\n",
    "\n",
    "def clean_response(text: str) -> str:\n",
    "    \"\"\"Clean up model response by removing XML-like tags and extra whitespace\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Remove XML-like tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove multiple spaces and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Clean up extra whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def setup_llm(config):\n",
    "    \"\"\"Initialize Ollama LLM\"\"\"\n",
    "    logger.info(\"ü§ñ Setting up Ollama LLM...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        llm = Ollama(\n",
    "            model=config.OLLAMA_MODEL,\n",
    "            temperature=config.TEMPERATURE,\n",
    "            num_ctx=config.NUM_CTX,\n",
    "            num_predict=512,  # Limit response length\n",
    "            top_k=config.TOP_K,\n",
    "            top_p=config.TOP_P,\n",
    "            repeat_penalty=1.1,  # Prevent repetitive responses\n",
    "            format=\"json\"  # Force structured output\n",
    "        )\n",
    "\n",
    "        # Test the LLM connection\n",
    "        test_response = llm.invoke(\"Hello, are you working?\")\n",
    "        \n",
    "        setup_time = time.time() - start_time\n",
    "        logger.info(f\"‚úÖ LLM setup completed in {setup_time:.2f} seconds\")\n",
    "        logger.info(f\"üéØ Model: {config.OLLAMA_MODEL} (GPU-accelerated)\")\n",
    "        logger.info(f\"üå°Ô∏è  Temperature: {config.TEMPERATURE}\")\n",
    "        \n",
    "        return llm\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error setting up LLM: {str(e)}\")\n",
    "        logger.error(\"üîß Make sure Ollama is running: ollama serve\")\n",
    "        raise\n",
    "\n",
    "def create_prompt_template():\n",
    "    \"\"\"Create custom prompt template for GNIDP-focused responses\"\"\"\n",
    "    \n",
    "    template = \"\"\"You are an expert assistant specialized in Little Andaman Island, its development projects, infrastructure, demographics, and all matters related to the Andaman & Nicobar Islands administration.\n",
    "\n",
    "        Gather information only from the provided context and documents to give a proper structured answer to the queries. \n",
    "        Context from Knowledge Base: {context}\n",
    "\n",
    "        User Question: {question}\n",
    "        Answer:\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    logger.info(\"üìù Custom prompt template created\")\n",
    "    return prompt\n",
    "\n",
    "# Setup LLM\n",
    "print(\"ü§ñ Initializing Ollama LLM...\")\n",
    "llm = setup_llm(config)\n",
    "\n",
    "# Create prompt template\n",
    "print(\"üìù Creating custom prompt template...\")\n",
    "prompt_template = create_prompt_template()\n",
    "\n",
    "print(\"‚úÖ LLM and prompt template ready!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eca0bee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:51:17,505 - INFO - üîó Creating Retrieval QA chain...\n",
      "2025-06-18 14:51:17,506 - INFO - ‚úÖ QA chain created successfully in 0.00 seconds\n",
      "2025-06-18 14:51:17,507 - INFO - üîç Retrieval documents: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Creating Retrieval QA chain...\n",
      "‚úÖ QA chain is ready!\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# MODULE 7: QA CHAIN CREATION\n",
    "# =====================================\n",
    "# Create the Retrieval QA chain that combines everything\n",
    "\n",
    "def create_qa_chain(llm, vectorstore, prompt_template, config):\n",
    "    \"\"\"Create the QA chain with custom prompt\"\"\"\n",
    "    logger.info(\"üîó Creating Retrieval QA chain...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create retrieval QA chain\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={\"k\": config.RETRIEVAL_K}\n",
    "            ),\n",
    "            chain_type_kwargs={\"prompt\": prompt_template},\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        \n",
    "        setup_time = time.time() - start_time\n",
    "        logger.info(f\"‚úÖ QA chain created successfully in {setup_time:.2f} seconds\")\n",
    "        logger.info(f\"üîç Retrieval documents: {config.RETRIEVAL_K}\")\n",
    "        \n",
    "        return qa_chain\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error creating QA chain: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def is_gnidp_related(question, keywords):\n",
    "    \"\"\"Check if question is related to GNIDP topics\"\"\"\n",
    "    question_lower = question.lower()\n",
    "    return any(keyword in question_lower for keyword in keywords)\n",
    "\n",
    "# Create QA Chain\n",
    "print(\"üîó Creating Retrieval QA chain...\")\n",
    "qa_chain = create_qa_chain(llm, vectorstore, prompt_template, config)\n",
    "print(\"‚úÖ QA chain is ready!\")\n",
    "# ====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56722234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODULE 8: QUERY INTERFACE AND TESTING\n",
    "# =====================================\n",
    "# Interactive query system and comprehensive testing\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "\n",
    "class GNIDPQuerySystem:\n",
    "    \"\"\"Complete query system for GNIDP RAG chatbot\"\"\"\n",
    "    \n",
    "    def __init__(self, qa_chain, config):\n",
    "        self.qa_chain = qa_chain\n",
    "        self.config = config\n",
    "        self.query_count = 0\n",
    "        self.total_response_time = 0\n",
    "        self.cache = QueryCache(\n",
    "            cache_dir=config.CACHE_DIR,\n",
    "            ttl=config.CACHE_TTL\n",
    "        ) if config.ENABLE_CACHE else None\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "\n",
    "    def query(self, question: str, format_output: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Process a query and return comprehensive response\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.query_count += 1\n",
    "        \n",
    "        if format_output:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üîç QUERY #{self.query_count}: {question}\")\n",
    "            print(f\"{'='*60}\")\n",
    "        \n",
    "        # Try cache first if enabled\n",
    "        if self.config.ENABLE_CACHE:\n",
    "            cached_result = self.cache.get(question)\n",
    "            if cached_result:\n",
    "                self.cache_hits += 1\n",
    "                response_time = time.time() - start_time\n",
    "                if format_output:\n",
    "                    print(f\"üöÄ Cache hit! Response time: {response_time:.2f}s\")\n",
    "                return cached_result\n",
    "\n",
    "        # Cache miss - process query normally\n",
    "        if self.config.ENABLE_CACHE:\n",
    "            self.cache_misses += 1\n",
    "        \n",
    "        # Pre-filter for GNIDP relevance\n",
    "        if not is_gnidp_related(question, self.config.GNIDP_KEYWORDS):\n",
    "            response_time = time.time() - start_time\n",
    "            result = {\n",
    "                \"answer\": \"I can only answer questions related to Little Andaman only. Please ask a question about these subjects.\",\n",
    "                \"response_time\": response_time,\n",
    "                \"relevant\": False,\n",
    "                \"query_number\": self.query_count,\n",
    "                \"cached\": False\n",
    "            }\n",
    "            \n",
    "            if format_output:\n",
    "                print(f\"‚ùå Not GNIDP-related\")\n",
    "                print(f\"ü§ñ Response: {result['answer']}\")\n",
    "                print(f\"‚è±Ô∏è  Response time: {response_time:.2f}s\")\n",
    "\n",
    "            # Cache the result if enabled\n",
    "            if self.config.ENABLE_CACHE:\n",
    "                self.cache.set(question, result)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        try:\n",
    "            # Get response from QA chain\n",
    "            if format_output:\n",
    "                print(f\"üîç Searching vector database...\")\n",
    "            qa_result = self.qa_chain.invoke({\"query\": question})\n",
    "            \n",
    "            response_time = time.time() - start_time\n",
    "            self.total_response_time += response_time\n",
    "            \n",
    "            # Clean the response\n",
    "            cleaned_answer = clean_response(qa_result[\"result\"])\n",
    "            \n",
    "            result = {\n",
    "                \"answer\": cleaned_answer,\n",
    "                \"response_time\": response_time,\n",
    "                \"relevant\": True,\n",
    "                \"query_number\": self.query_count,\n",
    "                \"cached\": False\n",
    "            }\n",
    "            \n",
    "            # Cache the result if enabled\n",
    "            if self.config.ENABLE_CACHE:\n",
    "                self.cache.set(question, result)\n",
    "            \n",
    "            if format_output:\n",
    "                # Display results\n",
    "                print(f\"‚úÖ GNIDP-related query processed\")\n",
    "                print(f\"\\nü§ñ ANSWER:\")\n",
    "                print(\"-\" * 50)\n",
    "                print(result[\"answer\"])\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                print(f\"‚è±Ô∏è  Response time: {response_time:.2f}s\")\n",
    "                print(f\"üìä Average response time: {self.total_response_time/self.query_count:.2f}s\")\n",
    "                \n",
    "                if self.config.ENABLE_CACHE:\n",
    "                    total_queries = self.cache_hits + self.cache_misses\n",
    "                    hit_rate = (self.cache_hits / total_queries * 100) if total_queries > 0 else 0\n",
    "                    print(f\"üíæ Cache Stats - Hit Rate: {hit_rate:.1f}% ({self.cache_hits}/{total_queries})\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            response_time = time.time() - start_time\n",
    "            error_result = {\n",
    "                \"answer\": f\"I encountered an error processing your question: {str(e)}\",\n",
    "                \"response_time\": response_time,\n",
    "                \"relevant\": True,\n",
    "                \"error\": str(e),\n",
    "                \"query_number\": self.query_count,\n",
    "                \"cached\": False\n",
    "            }\n",
    "            \n",
    "            if format_output:\n",
    "                print(f\"‚ùå ERROR: {str(e)}\")\n",
    "                print(f\"‚è±Ô∏è  Response time: {response_time:.2f}s\")\n",
    "            return error_result\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache performance statistics\"\"\"\n",
    "        if not self.config.ENABLE_CACHE:\n",
    "            return {\"cache_enabled\": False}\n",
    "            \n",
    "        total_queries = self.cache_hits + self.cache_misses\n",
    "        hit_rate = (self.cache_hits / total_queries * 100) if total_queries > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"cache_enabled\": True,\n",
    "            \"cache_hits\": self.cache_hits,\n",
    "            \"cache_misses\": self.cache_misses,\n",
    "            \"hit_rate\": f\"{hit_rate:.2f}%\",\n",
    "            **self.cache.get_stats()\n",
    "        }\n",
    "        \n",
    "    def batch_query(self, questions: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process multiple queries efficiently\"\"\"\n",
    "        results = []\n",
    "        print(f\"\\nüöÄ BATCH PROCESSING {len(questions)} QUERIES\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for question in questions:\n",
    "            result = self.query(question)\n",
    "            results.append(result)\n",
    "        \n",
    "        # Summary statistics\n",
    "        successful_queries = [r for r in results if 'error' not in r]\n",
    "        avg_time = sum(r['response_time'] for r in results) / len(results)\n",
    "        \n",
    "        print(f\"\\nüìä BATCH SUMMARY:\")\n",
    "        print(f\"Total queries: {len(questions)}\")\n",
    "        print(f\"Successful: {len(successful_queries)}\")\n",
    "        print(f\"Average response time: {avg_time:.2f}s\")\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba3cce3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing GNIDP Query Systems...\n",
      "‚úÖ Basic query system ready!\n",
      "üîÑ Upgrading Query System with Chat History...\n",
      "‚úÖ Enhanced Query System with Chat History ready!\n",
      "\n",
      "üéØ TESTING COMPLETE!\n",
      "‚úÖ System is fully operational and ready for use!\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# MODULE 8.5: CHAT HISTORY FUNCTIONALITY\n",
    "# =====================================\n",
    "\n",
    "@dataclass\n",
    "class ChatMessage:\n",
    "    \"\"\"Represents a single message in the chat history\"\"\"\n",
    "    role: str  # 'user' or 'assistant'\n",
    "    content: str\n",
    "    timestamp: datetime\n",
    "    metadata: Dict[str, Any] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n",
    "        return {\n",
    "            'role': self.role,\n",
    "            'content': self.content,\n",
    "            'timestamp': self.timestamp.isoformat(),\n",
    "            'metadata': self.metadata or {}\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict[str, Any]) -> 'ChatMessage':\n",
    "        \"\"\"Create from dictionary\"\"\"\n",
    "        return cls(\n",
    "            role=data['role'],\n",
    "            content=data['content'],\n",
    "            timestamp=datetime.fromisoformat(data['timestamp']),\n",
    "            metadata=data.get('metadata', {})\n",
    "        )\n",
    "\n",
    "class ChatHistoryManager:\n",
    "    \"\"\"Manages chat history with context awareness for GNIDP RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, config, max_history: int = 50, context_window: int = 5):\n",
    "        self.config = config\n",
    "        self.max_history = max_history\n",
    "        self.context_window = context_window  # Number of recent messages to include in context\n",
    "        self.chat_history: List[ChatMessage] = []\n",
    "        self.session_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.history_file = os.path.join(config.CACHE_DIR, f\"chat_history_{self.session_id}.json\")\n",
    "        \n",
    "        # Create history directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(self.history_file), exist_ok=True)\n",
    "        \n",
    "        # Load existing history if available\n",
    "        self._load_history()\n",
    "    \n",
    "    def add_message(self, role: str, content: str, metadata: Dict[str, Any] = None) -> None:\n",
    "        \"\"\"Add a new message to chat history\"\"\"\n",
    "        message = ChatMessage(\n",
    "            role=role,\n",
    "            content=content,\n",
    "            timestamp=datetime.now(),\n",
    "            metadata=metadata or {}\n",
    "        )\n",
    "        \n",
    "        self.chat_history.append(message)\n",
    "        \n",
    "        # Maintain max history limit\n",
    "        if len(self.chat_history) > self.max_history:\n",
    "            self.chat_history = self.chat_history[-self.max_history:]\n",
    "        \n",
    "        # Auto-save after each message\n",
    "        self._save_history()\n",
    "    \n",
    "    def get_context_for_query(self, current_query: str) -> str:\n",
    "        \"\"\"Get relevant context from chat history for the current query\"\"\"\n",
    "        if not self.chat_history:\n",
    "            return \"\"\n",
    "        \n",
    "        # Get recent messages within context window\n",
    "        recent_messages = self.chat_history[-self.context_window:]\n",
    "        \n",
    "        # Build context string\n",
    "        context_parts = []\n",
    "        for msg in recent_messages:\n",
    "            if msg.role == 'user':\n",
    "                context_parts.append(f\"Previous Question: {msg.content}\")\n",
    "            elif msg.role == 'assistant':\n",
    "                # Truncate long responses for context\n",
    "                content = msg.content[:300] + \"...\" if len(msg.content) > 300 else msg.content\n",
    "                context_parts.append(f\"Previous Answer: {content}\")\n",
    "        \n",
    "        if context_parts:\n",
    "            context = \"Recent conversation context:\\n\" + \"\\n\".join(context_parts) + \"\\n\\n\"\n",
    "            return context\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def get_conversation_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of current conversation\"\"\"\n",
    "        if not self.chat_history:\n",
    "            return {\"total_messages\": 0, \"session_id\": self.session_id}\n",
    "        \n",
    "        user_messages = [msg for msg in self.chat_history if msg.role == 'user']\n",
    "        assistant_messages = [msg for msg in self.chat_history if msg.role == 'assistant']\n",
    "        \n",
    "        return {\n",
    "            \"session_id\": self.session_id,\n",
    "            \"total_messages\": len(self.chat_history),\n",
    "            \"user_messages\": len(user_messages),\n",
    "            \"assistant_messages\": len(assistant_messages),\n",
    "            \"start_time\": self.chat_history[0].timestamp.isoformat() if self.chat_history else None,\n",
    "            \"last_activity\": self.chat_history[-1].timestamp.isoformat() if self.chat_history else None,\n",
    "            \"conversation_duration\": self._get_conversation_duration()\n",
    "        }\n",
    "    \n",
    "    def _get_conversation_duration(self) -> str:\n",
    "        \"\"\"Calculate conversation duration\"\"\"\n",
    "        if len(self.chat_history) < 2:\n",
    "            return \"0 minutes\"\n",
    "        \n",
    "        start_time = self.chat_history[0].timestamp\n",
    "        end_time = self.chat_history[-1].timestamp\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        total_minutes = int(duration.total_seconds() / 60)\n",
    "        hours = total_minutes // 60\n",
    "        minutes = total_minutes % 60\n",
    "        \n",
    "        if hours > 0:\n",
    "            return f\"{hours}h {minutes}m\"\n",
    "        else:\n",
    "            return f\"{minutes}m\"\n",
    "    \n",
    "    def display_history(self, limit: int = 10) -> None:\n",
    "        \"\"\"Display recent chat history\"\"\"\n",
    "        if not self.chat_history:\n",
    "            print(\"üìù No chat history available\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüí¨ CHAT HISTORY (Last {min(limit, len(self.chat_history))} messages)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        recent_messages = self.chat_history[-limit:]\n",
    "        \n",
    "        for i, msg in enumerate(recent_messages, 1):\n",
    "            role_emoji = \"üßë\" if msg.role == 'user' else \"ü§ñ\"\n",
    "            timestamp = msg.timestamp.strftime(\"%H:%M:%S\")\n",
    "            \n",
    "            print(f\"\\n{role_emoji} {msg.role.upper()} [{timestamp}]:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Truncate long messages for display\n",
    "            content = msg.content[:500] + \"...\" if len(msg.content) > 500 else msg.content\n",
    "            print(content)\n",
    "            \n",
    "            if msg.metadata:\n",
    "                print(f\"üìä Metadata: {msg.metadata}\")\n",
    "    \n",
    "    def search_history(self, query: str, limit: int = 5) -> List[ChatMessage]:\n",
    "        \"\"\"Search chat history for relevant messages\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        relevant_messages = []\n",
    "        \n",
    "        for msg in self.chat_history:\n",
    "            if query_lower in msg.content.lower():\n",
    "                relevant_messages.append(msg)\n",
    "        \n",
    "        return relevant_messages[-limit:]  # Return most recent matches\n",
    "    \n",
    "    def clear_history(self) -> None:\n",
    "        \"\"\"Clear all chat history\"\"\"\n",
    "        self.chat_history.clear()\n",
    "        self._save_history()\n",
    "        print(\"üóëÔ∏è Chat history cleared\")\n",
    "    \n",
    "    def export_history(self, filename: str = None) -> str:\n",
    "        \"\"\"Export chat history to a file\"\"\"\n",
    "        if filename is None:\n",
    "            filename = f\"gnidp_chat_export_{self.session_id}.json\"\n",
    "        \n",
    "        export_data = {\n",
    "            \"session_info\": self.get_conversation_summary(),\n",
    "            \"messages\": [msg.to_dict() for msg in self.chat_history]\n",
    "        }\n",
    "        \n",
    "        filepath = os.path.join(self.config.CACHE_DIR, filename)\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üìÑ Chat history exported to: {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def _save_history(self) -> None:\n",
    "        \"\"\"Save chat history to file\"\"\"\n",
    "        try:\n",
    "            export_data = {\n",
    "                \"session_info\": self.get_conversation_summary(),\n",
    "                \"messages\": [msg.to_dict() for msg in self.chat_history]\n",
    "            }\n",
    "            \n",
    "            with open(self.history_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to save chat history: {e}\")\n",
    "    \n",
    "    def _load_history(self) -> None:\n",
    "        \"\"\"Load chat history from file\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.history_file):\n",
    "                with open(self.history_file, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                self.chat_history = [\n",
    "                    ChatMessage.from_dict(msg_data) \n",
    "                    for msg_data in data.get('messages', [])\n",
    "                ]\n",
    "                \n",
    "                print(f\"Loaded {len(self.chat_history)} messages from history\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to load chat history: {e}\")\n",
    "            self.chat_history = []\n",
    "\n",
    "# Enhanced Query System with Chat History\n",
    "class GNIDPQuerySystemWithHistory(GNIDPQuerySystem):\n",
    "    \"\"\"Enhanced query system with chat history support\"\"\"\n",
    "    \n",
    "    def __init__(self, qa_chain, config):\n",
    "        super().__init__(qa_chain, config)\n",
    "        self.chat_manager = ChatHistoryManager(config)\n",
    "        \n",
    "        # Enhanced prompt template with history context\n",
    "        self.history_aware_template = \"\"\"You are an expert assistant specialized in Little Andaman Island, its development projects, infrastructure, demographics, and all matters related to the Andaman & Nicobar Islands administration.\n",
    "\n",
    "{chat_context}\n",
    "\n",
    "Context from Knowledge Base: {context}\n",
    "\n",
    "Current User Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Use the recent conversation context to better understand the current question\n",
    "- If the question refers to previous topics, incorporate that context in your response\n",
    "- Provide detailed, accurate information based on the knowledge base\n",
    "- If the question is a follow-up or clarification, acknowledge the connection to previous discussion\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Create PromptTemplate (assuming it's imported from langchain)\n",
    "        from langchain.prompts import PromptTemplate\n",
    "        self.history_prompt = PromptTemplate(\n",
    "            template=self.history_aware_template,\n",
    "            input_variables=[\"chat_context\", \"context\", \"question\"]\n",
    "        )\n",
    "    \n",
    "    def query_with_history(self, question: str, format_output: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Process query with chat history context\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.query_count += 1\n",
    "        \n",
    "        # Add user message to history\n",
    "        self.chat_manager.add_message(\"user\", question)\n",
    "        \n",
    "        if format_output:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üîç QUERY #{self.query_count}: {question}\")\n",
    "            print(f\"{'='*60}\")\n",
    "        \n",
    "        # Check cache first (if enabled)\n",
    "        if self.config.ENABLE_CACHE:\n",
    "            # Create cache key that includes recent context for better caching\n",
    "            recent_context = self.chat_manager.get_context_for_query(question)\n",
    "            cache_key = f\"{recent_context}||{question}\"\n",
    "            cached_result = self.cache.get(cache_key)\n",
    "            \n",
    "            if cached_result:\n",
    "                self.cache_hits += 1\n",
    "                response_time = time.time() - start_time\n",
    "                \n",
    "                # Add cached response to history\n",
    "                self.chat_manager.add_message(\"assistant\", cached_result[\"answer\"], {\n",
    "                    \"cached\": True,\n",
    "                    \"response_time\": response_time\n",
    "                })\n",
    "                \n",
    "                if format_output:\n",
    "                    print(f\"üöÄ Cache hit! Response time: {response_time:.2f}s\")\n",
    "                return cached_result\n",
    "        \n",
    "        # Cache miss - process with history context\n",
    "        if self.config.ENABLE_CACHE:\n",
    "            self.cache_misses += 1\n",
    "        \n",
    "        # Pre-filter for GNIDP relevance\n",
    "        if not is_gnidp_related(question, self.config.GNIDP_KEYWORDS):\n",
    "            response_time = time.time() - start_time\n",
    "            answer = \"I can only answer questions related to Little Andaman Island and GNIDP. Please ask a question about these subjects.\"\n",
    "            \n",
    "            result = {\n",
    "                \"answer\": answer,\n",
    "                \"response_time\": response_time,\n",
    "                \"relevant\": False,\n",
    "                \"query_number\": self.query_count,\n",
    "                \"cached\": False\n",
    "            }\n",
    "            \n",
    "            # Add to history\n",
    "            self.chat_manager.add_message(\"assistant\", answer, {\n",
    "                \"relevant\": False,\n",
    "                \"response_time\": response_time\n",
    "            })\n",
    "            \n",
    "            if format_output:\n",
    "                print(f\"‚ùå Not GNIDP-related\")\n",
    "                print(f\"ü§ñ Response: {answer}\")\n",
    "                print(f\"‚è±Ô∏è Response time: {response_time:.2f}s\")\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        try:\n",
    "            # Get chat context\n",
    "            chat_context = self.chat_manager.get_context_for_query(question)\n",
    "            \n",
    "            if format_output:\n",
    "                print(f\"üîç Searching vector database with history context...\")\n",
    "            \n",
    "            # Get relevant documents\n",
    "            docs = self.qa_chain.retriever.get_relevant_documents(question)\n",
    "            context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "            \n",
    "            # Create history-aware prompt\n",
    "            formatted_prompt = self.history_prompt.format(\n",
    "                chat_context=chat_context,\n",
    "                context=context,\n",
    "                question=question\n",
    "            )\n",
    "            \n",
    "            # Get response from LLM\n",
    "            response = self.qa_chain.llm.invoke(formatted_prompt)\n",
    "            \n",
    "            response_time = time.time() - start_time\n",
    "            self.total_response_time += response_time\n",
    "            \n",
    "            # Clean the response\n",
    "            cleaned_answer = clean_response(response)\n",
    "            \n",
    "            result = {\n",
    "                \"answer\": cleaned_answer,\n",
    "                \"response_time\": response_time,\n",
    "                \"relevant\": True,\n",
    "                \"query_number\": self.query_count,\n",
    "                \"cached\": False,\n",
    "                \"context_used\": bool(chat_context)\n",
    "            }\n",
    "            \n",
    "            # Add to history\n",
    "            self.chat_manager.add_message(\"assistant\", cleaned_answer, {\n",
    "                \"relevant\": True,\n",
    "                \"response_time\": response_time,\n",
    "                \"context_used\": bool(chat_context)\n",
    "            })\n",
    "            \n",
    "            # Cache the result\n",
    "            if self.config.ENABLE_CACHE:\n",
    "                cache_key = f\"{chat_context}||{question}\"\n",
    "                self.cache.set(cache_key, result)\n",
    "            \n",
    "            if format_output:\n",
    "                print(f\"‚úÖ GNIDP-related query processed\")\n",
    "                if chat_context:\n",
    "                    print(f\"üìö Used conversation context from {self.chat_manager.context_window} recent messages\")\n",
    "                \n",
    "                print(f\"\\nü§ñ ANSWER:\")\n",
    "                print(\"-\" * 50)\n",
    "                print(result[\"answer\"])\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                print(f\"‚è±Ô∏è Response time: {response_time:.2f}s\")\n",
    "                print(f\"üìä Average response time: {self.total_response_time/self.query_count:.2f}s\")\n",
    "                \n",
    "                # Show conversation summary\n",
    "                summary = self.chat_manager.get_conversation_summary()\n",
    "                print(f\"üí¨ Conversation: {summary['total_messages']} messages, {summary['conversation_duration']}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            response_time = time.time() - start_time\n",
    "            error_msg = f\"I encountered an error processing your question: {str(e)}\"\n",
    "            \n",
    "            error_result = {\n",
    "                \"answer\": error_msg,\n",
    "                \"response_time\": response_time,\n",
    "                \"relevant\": True,\n",
    "                \"error\": str(e),\n",
    "                \"query_number\": self.query_count,\n",
    "                \"cached\": False\n",
    "            }\n",
    "            \n",
    "            # Add error to history\n",
    "            self.chat_manager.add_message(\"assistant\", error_msg, {\n",
    "                \"error\": True,\n",
    "                \"response_time\": response_time\n",
    "            })\n",
    "            \n",
    "            if format_output:\n",
    "                print(f\"‚ùå ERROR: {str(e)}\")\n",
    "                print(f\"‚è±Ô∏è Response time: {response_time:.2f}s\")\n",
    "            \n",
    "            return error_result\n",
    "    \n",
    "    def show_conversation_stats(self):\n",
    "        \"\"\"Display conversation statistics\"\"\"\n",
    "        summary = self.chat_manager.get_conversation_summary()\n",
    "        \n",
    "        print(f\"\\nüìä CONVERSATION STATISTICS\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Session ID: {summary['session_id']}\")\n",
    "        print(f\"Total Messages: {summary['total_messages']}\")\n",
    "        print(f\"User Questions: {summary['user_messages']}\")\n",
    "        print(f\"Assistant Responses: {summary['assistant_messages']}\")\n",
    "        print(f\"Duration: {summary['conversation_duration']}\")\n",
    "        print(f\"Average Response Time: {self.total_response_time/self.query_count:.2f}s\" if self.query_count > 0 else \"No queries yet\")\n",
    "        \n",
    "        if self.config.ENABLE_CACHE:\n",
    "            cache_stats = self.get_cache_stats()\n",
    "            print(f\"Cache Hit Rate: {cache_stats['hit_rate']}\")\n",
    "\n",
    "# Initialize both systems\n",
    "print(\"üöÄ Initializing GNIDP Query Systems...\")\n",
    "query_system = GNIDPQuerySystem(qa_chain, config)\n",
    "print(\"‚úÖ Basic query system ready!\")\n",
    "\n",
    "print(\"üîÑ Upgrading Query System with Chat History...\")\n",
    "history_query_system = GNIDPQuerySystemWithHistory(qa_chain, config)\n",
    "print(\"‚úÖ Enhanced Query System with Chat History ready!\")\n",
    "\n",
    "print(f\"\\nüéØ TESTING COMPLETE!\")\n",
    "print(f\"‚úÖ System is fully operational and ready for use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f03f6ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõ†Ô∏è  AVAILABLE FUNCTIONS:\n",
      "==================================================\n",
      "‚ùì quick_query(question)             - Ask without history\n",
      "‚ùì quick_query_with_history(question) - Ask with context\n",
      "üí¨ show_chat_history(limit=10)       - View recent messages\n",
      "üîç search_chat_history(query)        - Search conversation\n",
      "üìä conversation_stats()              - Show chat statistics\n",
      "üìÑ export_chat_history(filename)     - Export conversation\n",
      "üóëÔ∏è clear_chat_history()              - Clear all history\n",
      "üìä system_status()                   - Check system status\n",
      "üíæ save_conversation()               - Save chat history\n",
      "\n",
      "üéØ TESTING CHAT HISTORY FUNCTIONALITY:\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "Question: What is the GNIDP project about?\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "I can only answer questions related to Little Andaman Island and GNIDP. Please ask a question about these subjects.\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Question: What are the main environmental concerns with this project?\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "I can only answer questions related to Little Andaman Island and GNIDP. Please ask a question about these subjects.\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Question: How will these concerns be addressed?\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "I can only answer questions related to Little Andaman Island and GNIDP. Please ask a question about these subjects.\n",
      "--------------------------------------------------\n",
      "\n",
      "üìä CONVERSATION STATISTICS\n",
      "========================================\n",
      "Session ID: 20250618_145946\n",
      "Total Messages: 6\n",
      "User Questions: 3\n",
      "Assistant Responses: 3\n",
      "Duration: 0m\n",
      "Average Response Time: 0.00s\n",
      "Cache Hit Rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# MODULE 9: UTILITY FUNCTIONS\n",
    "# ====================================\n",
    "\n",
    "# Original quick query function (no history)\n",
    "def quick_query(question: str):\n",
    "    \"\"\"Quick query function with simplified display formatting\"\"\"\n",
    "    # Format_output=False to prevent duplicate processing output\n",
    "    result = query_system.query(question, format_output=False)\n",
    "    \n",
    "    try:\n",
    "        answer = result[\"answer\"]\n",
    "        if answer.startswith('{\"') and answer.endswith('}'):\n",
    "            import json\n",
    "            parsed = json.loads(answer)\n",
    "            answer = parsed.get(\"answer\", answer)\n",
    "    except:\n",
    "        answer = result[\"answer\"]\n",
    "    \n",
    "    # Single clean output\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(\"-\"*50)\n",
    "    print(answer)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Enhanced utility functions with history support\n",
    "def quick_query_with_history(question: str):\n",
    "    \"\"\"Quick query function with chat history support\"\"\"\n",
    "    result = history_query_system.query_with_history(question, format_output=False)\n",
    "    \n",
    "    try:\n",
    "        answer = result[\"answer\"]\n",
    "        if answer.startswith('{\"') and answer.endswith('}'):\n",
    "            import json\n",
    "            parsed = json.loads(answer)\n",
    "            answer = parsed.get(\"answer\", answer)\n",
    "    except:\n",
    "        answer = result[\"answer\"]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(\"-\"*50)\n",
    "    print(answer)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    if result.get(\"context_used\"):\n",
    "        print(\"üìö Used previous conversation context\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def show_chat_history(limit: int = 10):\n",
    "    \"\"\"Show recent chat history\"\"\"\n",
    "    history_query_system.chat_manager.display_history(limit)\n",
    "\n",
    "def search_chat_history(query: str):\n",
    "    \"\"\"Search chat history for specific content\"\"\"\n",
    "    results = history_query_system.chat_manager.search_history(query)\n",
    "    \n",
    "    if not results:\n",
    "        print(f\"üîç No messages found containing '{query}'\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüîç SEARCH RESULTS for '{query}' ({len(results)} matches)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for msg in results:\n",
    "        role_emoji = \"üßë\" if msg.role == 'user' else \"ü§ñ\"\n",
    "        timestamp = msg.timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        print(f\"\\n{role_emoji} {msg.role.upper()} [{timestamp}]:\")\n",
    "        content = msg.content[:300] + \"...\" if len(msg.content) > 300 else msg.content\n",
    "        print(content)\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "def export_chat_history(filename: str = None):\n",
    "    \"\"\"Export chat history to file\"\"\"\n",
    "    return history_query_system.chat_manager.export_history(filename)\n",
    "\n",
    "def clear_chat_history():\n",
    "    \"\"\"Clear all chat history\"\"\"\n",
    "    history_query_system.chat_manager.clear_history()\n",
    "\n",
    "def conversation_stats():\n",
    "    \"\"\"Show conversation statistics\"\"\"\n",
    "    history_query_system.show_conversation_stats()\n",
    "    \n",
    "# Utility functions\n",
    "def system_status():\n",
    "    \"\"\"Display current system status\"\"\"\n",
    "    print(\"\\nüîç SYSTEM STATUS CHECK\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Check components\n",
    "    components = {\n",
    "        \"üìÑ Documents\": len(documents) if 'documents' in globals() and documents is not None else \"Using existing vector store\",\n",
    "        \"üß† Embeddings\": \"‚úÖ Loaded\" if 'embeddings' in globals() else \"‚ùå Not loaded\",\n",
    "        \"üóÉÔ∏è  Vector Store\": \"‚úÖ Ready\" if 'vectorstore' in globals() else \"‚ùå Not ready\",\n",
    "        \"ü§ñ LLM\": \"‚úÖ Connected\" if 'llm' in globals() else \"‚ùå Not connected\",\n",
    "        \"üîó QA Chain\": \"‚úÖ Ready\" if 'qa_chain' in globals() else \"‚ùå Not ready\"\n",
    "    }\n",
    "    \n",
    "    for component, status in components.items():\n",
    "        print(f\"{component}: {status}\")\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "    print(f\"   Model: {config.OLLAMA_MODEL}\")\n",
    "    print(f\"   Vector Store: {config.VECTORSTORE_TYPE}\")\n",
    "    print(f\"   Chunk Size: {config.CHUNK_SIZE}\")\n",
    "    print(f\"   Retrieval K: {config.RETRIEVAL_K}\")\n",
    "    \n",
    "    if query_system.query_count > 0:\n",
    "        print(f\"\\nüìä Performance:\")\n",
    "        print(f\"   Queries processed: {query_system.query_count}\")\n",
    "        print(f\"   Average response time: {query_system.total_response_time/query_system.query_count:.2f}s\")\n",
    "\n",
    "def save_conversation(conversation_log: List[Dict], filename: str = None):\n",
    "    \"\"\"Save conversation history to file\"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"gnidp_conversation_{timestamp}.txt\"\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"GNIDP RAG Chatbot Conversation Log\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        \n",
    "        for i, entry in enumerate(conversation_log, 1):\n",
    "            f.write(f\"Query {i}: {entry['question']}\\n\")\n",
    "            f.write(f\"Response: {entry['answer']}\\n\")\n",
    "            f.write(f\"Response Time: {entry['response_time']:.2f}s\\n\")\n",
    "            f.write(f\"Relevant: {entry['relevant']}\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"üíæ Conversation saved to {filename}\")\n",
    "\n",
    "# Display available functions\n",
    "print(\"\\nüõ†Ô∏è  AVAILABLE FUNCTIONS:\")\n",
    "print(\"=\"*50)\n",
    "print(\"‚ùì quick_query(question)             - Ask without history\")\n",
    "print(\"‚ùì quick_query_with_history(question) - Ask with context\")\n",
    "print(\"üí¨ show_chat_history(limit=10)       - View recent messages\")\n",
    "print(\"üîç search_chat_history(query)        - Search conversation\")\n",
    "print(\"üìä conversation_stats()              - Show chat statistics\")\n",
    "print(\"üìÑ export_chat_history(filename)     - Export conversation\")\n",
    "print(\"üóëÔ∏è clear_chat_history()              - Clear all history\")\n",
    "print(\"üìä system_status()                   - Check system status\")\n",
    "print(\"üíæ save_conversation()               - Save chat history\")\n",
    "\n",
    "print(\"\\nüéØ TESTING CHAT HISTORY FUNCTIONALITY:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test conversation with context\n",
    "quick_query_with_history(\"What is the GNIDP project about?\")\n",
    "quick_query_with_history(\"What are the main environmental concerns with this project?\")\n",
    "quick_query_with_history(\"How will these concerns be addressed?\")\n",
    "\n",
    "# Show conversation stats\n",
    "conversation_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "142cdc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: Total Revenue land in little andaman\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "836.8144 hectares\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "quick_query(\"Total Revenue land in little andaman\")  # Another example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd9ecb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: How many buses are there in little andaman?\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "15\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "quick_query(\"How many buses are there in little andaman?\")  # Another example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e37186d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: What is the vision of little andaman development project?\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "{ \"answer\": \"The vision of the Little Andaman Development Project is to provide full IT support and implement e-Gov Services in the A&N Islands, aiming to overcome digital divide with the rest of India.\" }\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "quick_query(\"What is the vision of little andaman development project?\")  # Another example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc5cdf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_system.cache.clear()  # Clear cache if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad1e4ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: How much of the land in little andaman is reserved forest?\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "The total area of the land in Little Andaman includes reserved forests, protected forests, and unclassified forests. The reserved forest area is 706.49 square kilometers.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "quick_query(\"How much of the land in little andaman is reserved forest?\")  # Another example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0708399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: Little andaman total land?\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "I encountered an error processing your question: 'RetrievalQA' object has no attribute 'llm'\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "quick_query_with_history(\"Little andaman total land?\")  # Another example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b0566ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: Are there schools in little andaman? Id so, how many?\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "I encountered an error processing your question: 'RetrievalQA' object has no attribute 'llm'\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "quick_query_with_history(\"Are there schools in little andaman? Id so, how many?\")  # Another example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ae42a786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: Distance form little andaman\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "I encountered an error processing your question: 'RetrievalQA' object has no attribute 'llm'\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "quick_query_with_history(\"Distance form little andaman\")  # Another example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "00715a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question:  sea distance between port blair & little andaman\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "I encountered an error processing your question: 'RetrievalQA' object has no attribute 'llm'\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "quick_query_with_history(\" sea distance between port blair & little andaman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e3d3590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: How many primary schools in little andaman?\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "I encountered an error processing your question: 'RetrievalQA' object has no attribute 'llm'\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "quick_query_with_history(\"How many primary schools in little andaman?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c936ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
